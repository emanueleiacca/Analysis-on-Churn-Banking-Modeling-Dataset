WARNING: Spiegare meglio metric choice 

START
Prima di tutto, l'Analisi esplorativa e la Feature Engineering eseguita sul Dataset è servita
a individuare eventuali errori nella raccolta dei dati e a modificare alcune variabili in modo 
da migliorare il modello. Abbiamo deciso di rimuovere Id_Cliente e poi 3 variabili correlate 
oltre lo 0.9, mantenendo poi il dataset iniziale quasi grezzo, effettuando solo l'encoding di 
alcune variabili categoriche. Questa scelta è stata presa in relazione all'obiettivo di analisi, 
in modo da perdere meno informazione possibile. Anche i valori mancanti non sono stati trattati, 
in quanto gli algoritmi di boosting utilizzati sono stati progettati per gestirli in automatico.

SAMPLING METHOD CHOICE
Nella fase di preprocessing ci siamo posti la domanda di quale fosse il metodo per 
trattare le classi sbilanciate, lo abbiamo dimostrato empiricamente, provando:
-RandomUnderSampling, con l'undersampling si riduce il numero di campioni nella classe maggioritaria 
(la classe con più frequenze) in modo che il numero di campioni in ciascuna classe sia più bilanciato. 
Questo si fa rimuovendo casualmente un certo numero di campioni dalla classe maggioritaria fino a quando 
il numero di campioni in entrambe le classi non è approssimativamente lo stesso.
-RandomOverSampling, con l'oversampling si aumenta il numero di campioni nella classe minoritaria (la 
classe con meno istanze) replicando o generando artificialmente nuovi campioni fino a raggiungere un 
bilanciamento tra le classi.
-Semplice sampling con Cost-Learning function, dopo il campionamento casuale, la funzione entra in 
gioco. L'algoritmo di apprendimento viene addestrato per minimizzare una funzione di costo che tiene 
conto dei costi associati ai diversi tipi di errori: durante l'addestramento, il modello sarà ottimizzato 
per fare previsioni che minimizzano il costo complessivo, piuttosto che solo l'errore di classificazione. 
Questa funzione assegna un valore numerico che rappresenta il costo o la perdita associata a una previsione 
errata o a una decisione sbagliata. La cost-learning function è utilizzata per addestrare i modelli di 
apprendimento automatico al fine di minimizzare il costo complessivo delle previsioni errate. 
La combinazione di campionamento casuale e Cost-Learning function può portare a un modello che è più 
robusto rispetto allo sbilanciamento delle classi e che tiene conto dei costi associati agli errori di 
classificazione. Tuttavia, è importante considerare che il campionamento casuale potrebbe non 
selezionare istanze basate sui loro costi associati, quindi potrebbe essere inefficace nel catturare
completamente la complessità dei costi nel dataset.

L'ultimo metodo si è dimostrato il migliore ed anche il piu' semplice da implementare in Optuna, 
c'è ovviamente da sottolineare che questo metodo, cosi come tutte le tecniche di bilanciamento, 
suppone che il rapporto tra classe minoritaria e totale sia lo stesso.

OPTUNA
Optuna è un framework per automatizzare la ricerca degli iperparametri. 
Supera GridSearchCv perché è più veloce, in quanto utilizza più thread contemporaneamente, inoltre
è anche più efficiente, poiché possiamo provare ogni possibile configurazione di iperparametri in 
modo efficiente utilizzando TPE Sampler, un algoritmo di ottimizzazione degli iperparametri bayesiani
che apprende la relazione tra gli iperparametri e può darci i migliori risultati in un tempo più 
breve. Infatti abbiamo avuto bisogno solo di 50 prove per avere i migliori risultati possibili.

THE ALGORITHMS USED WITH OPTUNA TUNING
Per massimizzare i risultati abbiamo applicato anche degli algoritmi Boosting.
Boosting combina più algoritmi deboli come un albero decisionale per formarne uno più forte, di
seguito i metodi Boosting utilizzati:

-LightGBM, progettato da Google per grandi Dataset con molte variabili.
E' un Leaf-wise tree growth, ciò significa che l'albero cresce verticalmente una foglia alla volta,
per selezionare quella foglia riduciamo la funzione loss al minimo e la espandiamo finché il nostro 
albero non cattura qualsiasi modello e relazione nei dati, portando a una modello potente.

-XGBoost, progettato da un ricercatore cinese, funziona in modo simile ma utilizza i metodi Lasso 
e/o Ridge come penalità per prevenire l'overfitting e Tree Pruning per fermare la crescita 
dell'albero quando si aggiungono nuove foglie che non contribuiscono in modo significativo a 
ridurre la funzione loss(quindi quando è sotto una certa soglia): questo permette di avere
un albero più efficiente e meno complesso.

- Catboost:
CatBoost è un algoritmo di machine learning sviluppato da Yandex che si basa su un approccio iterativo per costruire un insieme di alberi decisionali deboli in modo sequenziale. 
Questo metodo prevede che ad ogni iterazione venga selezionato l'albero che minimizza la funzione di perdita complessiva.
 La funzione di perdita di CatBoost, chiamata CatBoost Loss Function, si basa su tecniche come l'Ordered Permutation e il Pairwise Comparison.
Una caratteristica distintiva di CatBoost è l'utilizzo di una tecnica di Pruning Regolarizzato per controllare la crescita degli alberi. 
Questo permette di evitare l'overfitting, migliorando la generalizzazione del modello. Inoltre, a differenza di altri algoritmi che utilizzano una soglia fissa per fermare la crescita degli alberi, 
CatBoost combina il guadagno di informazione con la complessità del modello per ottimizzare la struttura degli alberi.
CatBoost impiega un algoritmo di ricerca degli alberi più sofisticato, che si basa su tecniche come Column-wise Sampling e Greedy Feature Selection. 
Questo approccio consente di selezionare le caratteristiche più rilevanti e di costruire modelli più efficienti e precisi.

 
-Ensembling Weight Optimization, Determina il contributo ottimale dei singoli modelli all'interno di 
un insieme per migliorare le prestazioni predittive complessive. Attraverso aggiustamenti iterativi 
(utilizzando Optuna) assegniamo l'influenza appropriata a ciascun modello. L’obiettivo è quello di 
raggiungere un insieme equilibrato che sfrutti i punti di forza dei suoi componenti mitigando al 
contempo i potenziali punti deboli, portando a un miglioramento del potere predittivo complessivo.

Il metodo scelto è proprio quest'ultimo, che sembra avere performance migliori proprio perchè 
considera i diversi algoritmi assieme.

Durante l'applicazione di ogni algoritmo, sono stati eseguiti i seguenti passaggi:
-Split in train e test con random sampling
-Scelta dei parametri di tuning, tra cui la cost learning function
-Tuning dei parametri con Optuna, che ha permesso di osservare quali fossero i migliori parametri da 
utilizzare nel modello. I primi grafici riguardo la scelta dei parametri hanno dato le seguenti informazioni:
    -L'history plot in Optuna è un grafico che mostra come variano le prestazioni nel corso delle iterazioni di
    ottimizzazione, sull'asse x, ogni punto rappresenta uno specifico "trial" o iterazione del processo di 
    ricerca degli iperparametri. Interpretando l'history plot, puoi osservare come varia la funzione obiettivo 
    (ad esempio la loss o l'accuracy) al variare delle diverse combinazioni di iperparametri valutate durante 
    il processo di ottimizzazione. L'obiettivo è trovare la combinazione di iperparametri che massimizza o minimizza 
    la funzione obiettivo, indicando prestazioni ottimali del modello. La linea rossa collega alcuni punti nell'history 
    plot per mostrare la sequenza temporale delle iterazioni e come le prestazioni del modello si sono evolute nel corso 
    del tempo. 
    -L'hyperparameters importance, 'importanza dei parametri può essere utilizzata per identificare le caratteristiche più 
    influenti o rilevanti per il modello. Questo può aiutare a ridurre la dimensionalità del dataset, eliminando le 
    caratteristiche meno informative e riducendo così il rischio di overfitting e migliorando le prestazioni del modello.
    -Lo slice plot, è una visualizzazione che mostra come varia la predizione del modello al variare di un parametro, 
    mantenendo costanti tutti gli altri.
-Applicazione del modello con i parametri scelti grazie ad Optuna, è stata utilizzata anche la cross validation per 
ridurre l'overfitting ed avere un modello più stabile
-Valutazione dei risultati: osservazione di quanti 'si' sono stati selezionati correttamente nei primi 10k (i 10k sono
scelti tra i primi 10000 con maggiore probabilità di appartenere alla classe 'si' )

Infine, con la curva di apprendimento si è voluto verificare la capacità di generalizzare del modello: la capacità di 
generalizzazione di un modello si riferisce alla sua capacità di fare previsioni accurate su dati non visti, cioè su 
dati che non sono stati utilizzati durante il processo di addestramento. 
Se le curve convergono e mostrano prestazioni simili sia sui dati di addestramento che sui dati di test, ciò suggerisce 
che il modello generalizza bene e che non vi è overfitting. Se le curve rimangono distanti tra loro, con prestazioni 
significativamente migliori sui dati di addestramento rispetto ai dati di test, potrebbe essere indicativo di overfitting, 
cioè il modello si adatta troppo ai dati di addestramento e non generalizza bene su nuovi dati.



METRIC CHOICE
Si è provato anche ad individuare la metrica migliore, lavorando sull'algoritmo LGBM Boost (perchè 
è l'algoritmo più veloce tra quellli selezionati). E' stato eseguito il tuning con cross validation selezionando volta per 
volta la metrica da massimizzare e si sono esservati i risultati.

Abbiamo valutando le seguenti metriche:
-ROC AUC: empiricamente sembra funzionare bene ma considerando il nostro obiettivo di analisi 
potrebbero essere migliori altre metriche dal punto di vista logico: La curva ROC è utile per 
valutare le prestazioni del modello in generale e per confrontare diversi modelli, ma non 
necessariamente fornisce una misura diretta della capacità del modello di classificare correttamente
le istanze della classe di interesse. 
Inoltre, valutando le informazioni dedotte durante l'analisi esplorativa e applicando alcuni metodi
di Interpretable Machine Learning, abbiamo osservato come effettivamente la curva ROC da minore 
importanza a predittori che invece potrebbero essere più importanti nella previsione.
-Precision: consigliata dal professore, minimizza i FP cosa che a noi non interessa, i risultati anche qui
sembrano abbastanza buoni 
-Recall : massimizza i TP, il che teoricamente fa al caso nostro, ma i risultati
empirici sono peggiori rispetto a quelli della ROC AUC e della Precision
-FScore: bilanciando Recall e Precision (abbiamo provato F1 e F2 quindi con maggior peso su Recall) 
i risultati sono anche qui nella media
-Rank method: nessuna metrica, abbiamo semplicemente ordinato in base alle probabilità e selezionato 
il 1:6 dei migliori. E' stato considerato come score il rapporto di 'si' predetti su 'si' totali. 


Per il dataset originale i risultati sul Dataset sono i seguenti:
-Rank probabilities: Numero di 'si' 306/397, FN=85
-Roc Auc: Numero di 'si' 311/397,  FN=391
-Recall: Numero di 'si' 293/397, FN=81
-Precision: Numero di 'si' 310/397, FN=358
-F1: Numero di 'si' 302/397, FN=351
-F2: Numero di 'si' 308/397, FN=274

I risultati sul Dataset sintetico:
-Rank probabilities: Numero di 'si' 322/358, FN=36
-Roc Auc: Numero di 'si' 298/358, FN=356
-Recall: Numero di 'si' 295/358, FN=44
-Precision: Numero di 'si' 312/358, FN=303
-F1: Numero di 'si' 304/358, FN=291
-F2: Numero di 'si' 309/358, FN=215


Le metriche migliori sembrano essere Recall e Rank Probabilities, ciò rispecchia anche logicamente 
l'obiettivo di analisi, ossia individuare quanti più appartenenti alla classe 'si' veritieri. In 
questo modello bisogna dare più importanza ai falsi negativi. Inoltre, si è osservato che la 
trasformazione logaritmica non è stata utile a migliorare il modello, quindi tale processo è stato rimosso. 
La metrica scelta è Rank probabilities.


APPLICAZIONE DEGLI STESSI METODI SU UN DATASET SINTETICO
Per generare il dataset sintetico abbiamo usato ACTGAN, che è un modello ottimizzato del CTGAN, 
basato sul GAN(Generative Adversarial Network) che crea Dataset sintetici attraverso Neural Network.
Il funzionamento è molto semplice, ci sono due reti neurali che si sfidano a vicenda al fine di 
migliorarsi: la prima, il discriminante, ha il compito di creare nuovi dati dati al fine di ingannare
il discriminatore, ovvero la seconda NN che invece è un discriminatore che ha il compito di 
distinguere dati creati dal generatore e reali. 
Attraverso Gretel AI: https://github.com/gretelai/gretel-synthetics?tab=readme-ov-file
Abbiamo trainato il modello per creare il dataset sintetico. Gretel ci fornisce un set up ottimizato
del GAN che in particolare non necessitano di trattare i dati, il GAN originale necessava imputarli. 
Gretel AI fornisce un interfaccia user friendly per creare il dataset sintetico e che usa 
google colab per elaborare la nostra richiesta. Il modello di per se è molto pesante, 
nella documentazione infatti sono consigliati 32gb di RAM e GPU dedicata di nuova gen. 
Si è provato a trainare il modello sia in locale che in colab ma il processo era troppo lento. 
Fortunatamente l'interfaccia di cui si è parlato prima fornisce il collegamento con google colab dove 
si utilizzano GPU fornite a pagamento tramite Colab, il training è durato poco piu' di due ore.
In seguito al training è stato fornito un report che dimostra come il dataset sintetico creato
rispecchi l'andamento del dataset originale, pur avendo dati diversi. Una volta trainato, è possibile
scegliere il numero di righe richiesto per il dataset sintetico e generarlo usando il modello trainato 
in precedenza.

FROM BLACKBOX TO WHITEBOX
Per comprendere meglio il processo di addestramento dei 3 modelli utilizzati, abbiamo utilizzato delle 
tecniche di interpretable machine learning, utili a spiegare come il modello ha trainato i dati, ad esempio
a quali variabili ha dato più importanza, se queste hanno influito negativamente o positivamente alle previsioni.
- Shapley Values: Le variabili con un effetto negativo (rappresentato dal colore blu) hanno un'associazione inversa 
con la target. Ciò significa che valori di shapley più alti della variabile sono associati a una diminuzione della probabilità 
che un cliente richieda l'estinzione del conto corrente (CC) o abbandoni la banca, quindi che appartenga alla classe 'no'.
Il calcolo del valore di Shapley coinvolge i seguenti passaggi:

Come si calcolano i valori di Shapley: 
generazione di tutti i sottoinsiemi di feature: Per calcolare il valore di Shapley per una feature 
𝑖, dobbiamo considerare tutte le possibili combinazioni di feature che includono 𝑖 insieme a un insieme di feature 𝑆
diverse da 𝑖. Calcolo della contribuzione marginale: Per ogni combinazione di feature, calcoliamo la predizione del 
modello sia con che senza la feature 𝑖. La differenza tra la predizione del modello con 𝑖 e senza 𝑖 rappresenta la 
contribuzione marginale della feature 𝑖 in quella particolare combinazione di feature. 
La media delle contribuzioni marginale su tutte le combinazioni: Calcoliamo la media delle contribuzioni marginali su 
tutte le combinazioni di feature rappresenta il valore di Shapley per la feature 𝑖.



